/**
 * Airflow Export Generator
 * 
 * Generates Airflow DAG Python files from Twiddle IR.
 */

import type { TwiddleWorkflowIR } from './ir/index.js';
import { workflowToIR } from './ir/index.js';
import { irToAirflow, type AirflowDAG } from './export/airflow-adapter.js';

/**
 * Generate the main DAG file
 */
function generateDagFile(dag: AirflowDAG): string {
    const lines: string[] = [];

    // File header
    lines.push('"""');
    lines.push(`Airflow DAG: ${dag.dagId}`);
    if (dag.description) {
        lines.push('');
        lines.push(dag.description);
    }
    lines.push('');
    lines.push('Generated by Twiddle');
    lines.push('"""');
    lines.push('');

    // Imports
    for (const imp of dag.imports) {
        lines.push(imp);
    }
    lines.push('');

    // Default args
    lines.push('default_args = {');
    for (const [key, value] of Object.entries(dag.defaultArgs)) {
        if (typeof value === 'string' && value.startsWith('timedelta')) {
            lines.push(`    '${key}': ${value},`);
        } else {
            lines.push(`    '${key}': ${JSON.stringify(value)},`);
        }
    }
    lines.push('}');
    lines.push('');

    // Generate Python callables for PythonOperator tasks
    for (const task of dag.tasks) {
        if (task.operator === 'PythonOperator' || task.operator === 'BranchPythonOperator') {
            const callableName = task.kwargs.python_callable as string;
            lines.push(`def ${callableName}(**kwargs):`);
            lines.push(`    """${task.name}"""`);

            if (task.isBranch) {
                lines.push('    # Branch logic - return task_id to execute');
                lines.push('    # TODO: Implement branch condition');
                lines.push('    return None');
            } else {
                lines.push('    # TODO: Implement task logic');
                lines.push(`    params = ${JSON.stringify(task.kwargs.op_kwargs || {})}`);
                lines.push('    print(f"Executing {kwargs.get(\'task_instance\').task_id}")');
                lines.push('    return params');
            }
            lines.push('');
        }
    }

    // DAG definition
    lines.push(`with DAG(`);
    lines.push(`    dag_id='${dag.dagId}',`);
    if (dag.description) {
        lines.push(`    description='${dag.description.replace(/'/g, "\\'")}',`);
    }
    if (dag.scheduleInterval) {
        if (dag.scheduleInterval.startsWith('timedelta')) {
            lines.push(`    schedule_interval=${dag.scheduleInterval},`);
        } else {
            lines.push(`    schedule_interval='${dag.scheduleInterval}',`);
        }
    } else {
        lines.push(`    schedule_interval=None,`);
    }
    lines.push(`    start_date=datetime(2024, 1, 1),`);
    lines.push(`    catchup=${dag.catchup ? 'True' : 'False'},`);
    lines.push(`    default_args=default_args,`);
    if (dag.tags.length > 0) {
        lines.push(`    tags=${JSON.stringify(dag.tags)},`);
    }
    lines.push(`) as dag:`);
    lines.push('');

    // Task definitions
    for (const task of dag.tasks) {
        lines.push(`    ${task.taskId} = ${task.operator}(`);
        lines.push(`        task_id='${task.taskId}',`);

        // Add kwargs
        for (const [key, value] of Object.entries(task.kwargs)) {
            if (key === 'python_callable') {
                lines.push(`        ${key}=${value},`);
            } else if (typeof value === 'string' && value.startsWith('timedelta')) {
                lines.push(`        ${key}=${value},`);
            } else {
                lines.push(`        ${key}=${JSON.stringify(value)},`);
            }
        }

        // Retries
        if (task.retries > 0) {
            lines.push(`        retries=${task.retries},`);
        }

        // Execution timeout
        if (task.executionTimeout) {
            lines.push(`        execution_timeout=timedelta(seconds=${task.executionTimeout}),`);
        }

        lines.push(`    )`);
        lines.push('');
    }

    // Dependencies
    if (dag.dependencies.length > 0) {
        lines.push('    # Task dependencies');
        for (const dep of dag.dependencies) {
            lines.push(`    ${dep.upstream} >> ${dep.downstream}`);
        }
    }

    return lines.join('\n');
}

/**
 * Generate requirements.txt for Airflow
 */
function generateRequirements(dag: AirflowDAG): string {
    const requirements = new Set<string>();

    // Base Airflow
    requirements.add('apache-airflow>=2.0.0');

    // Check for provider packages
    for (const task of dag.tasks) {
        if (task.operatorModule.includes('providers.http')) {
            requirements.add('apache-airflow-providers-http');
        }
        if (task.operatorModule.includes('providers.ssh')) {
            requirements.add('apache-airflow-providers-ssh');
        }
        if (task.operatorModule.includes('providers.slack')) {
            requirements.add('apache-airflow-providers-slack');
        }
    }

    return Array.from(requirements).join('\n');
}

/**
 * Generate README for the Airflow project
 */
function generateReadme(dag: AirflowDAG): string {
    return `# ${dag.dagId}

${dag.description || 'Airflow DAG generated by Twiddle'}

## Setup

1. Install dependencies:
   \`\`\`bash
   pip install -r requirements.txt
   \`\`\`

2. Copy the DAG file to your Airflow dags folder:
   \`\`\`bash
   cp dag.py $AIRFLOW_HOME/dags/
   \`\`\`

3. Refresh Airflow:
   \`\`\`bash
   airflow dags reserialize
   \`\`\`

## Tasks

${dag.tasks.map(t => `- **${t.taskId}**: ${t.name} (${t.operator})`).join('\n')}

## Dependencies

${dag.dependencies.map(d => `\`${d.upstream}\` â†’ \`${d.downstream}\``).join('\n') || 'No dependencies'}

---

Generated by Twiddle
`;
}

/**
 * Workflow data for export (simplified)
 */
export interface WorkflowData {
    id: string;
    name: string;
    description?: string;
    nodes: unknown[];
    connections: unknown[];
}

/**
 * Generate Airflow DAG export from workflow data
 * 
 * @param workflow - Workflow data (DB format)
 * @returns Record of filename to content
 */
export function generateAirflowExport(workflow: WorkflowData): Record<string, string> {
    // Convert to IR
    const ir = workflowToIR({
        id: workflow.id,
        name: workflow.name,
        description: workflow.description,
        nodes: workflow.nodes,
        connections: workflow.connections,
    });

    // Convert to Airflow model
    const dag = irToAirflow(ir);

    // Generate files
    return {
        'dag.py': generateDagFile(dag),
        'requirements.txt': generateRequirements(dag),
        'README.md': generateReadme(dag),
    };
}

/**
 * Generate Airflow DAG from IR directly
 * 
 * @param ir - Twiddle Workflow IR
 * @returns Record of filename to content
 */
export function generateAirflowFromIR(ir: TwiddleWorkflowIR): Record<string, string> {
    const dag = irToAirflow(ir);

    return {
        'dag.py': generateDagFile(dag),
        'requirements.txt': generateRequirements(dag),
        'README.md': generateReadme(dag),
    };
}
